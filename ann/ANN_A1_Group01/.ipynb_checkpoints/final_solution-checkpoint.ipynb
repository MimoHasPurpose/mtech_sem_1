{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20f928e-b728-46e3-af52-f92bac1e0be8",
   "metadata": {},
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d235050-b813-4bf0-9d84-22eded7fe747",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4233861-1d43-4c7e-a802-f507fa3334e9",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cdeb7ca-114d-4b5a-9532-65b25b6b2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad3085fc-4ca7-4bc6-8e16-d8af74596940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset\n",
    "def load_cifar10(path):\n",
    "    data, labels = [], []\n",
    "    for i in range(1, 6):\n",
    "        with open(os.path.join(path, f\"data_batch_{i}\"), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='bytes')\n",
    "            data.append(batch[b'data'])\n",
    "            labels += batch[b'labels']\n",
    "    X = np.concatenate(data).astype(np.float32) / 255.0\n",
    "    y = np.array(labels).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def load_cifar10_test(path):\n",
    "    with open(os.path.join(path, \"test_batch\"), 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "        X = batch[b'data'].astype(np.float32) / 255.0\n",
    "        y = np.array(batch[b'labels']).reshape(-1, 1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa7d54c1-7c51-4930-a391-d8cdfbfc47c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN \n",
    "class ANN:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append({\n",
    "                \"W\": np.random.randn(prev, h) * 0.01,\n",
    "                \"b\": np.zeros((1, h))\n",
    "            })\n",
    "            prev = h\n",
    "        self.out = {\n",
    "            \"W\": np.random.randn(prev, output_dim) * 0.01,\n",
    "            \"b\": np.zeros((1, output_dim))\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, z): \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_deriv(self, a): \n",
    "        return a * (1 - a)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = [{'a': X}]\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.sigmoid(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer update\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            a_curr = self.cache[i+1]['a']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.sigmoid_deriv(a_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed70ba5-3a39-42c1-89ba-eee8160dd520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_r:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append({\n",
    "                \"W\": np.random.randn(prev, h) * np.sqrt(2.0 / prev),  # He init for ReLU\n",
    "                \"b\": np.zeros((1, h))\n",
    "            })\n",
    "            prev = h\n",
    "        self.out = {\n",
    "            \"W\": np.random.randn(prev, output_dim) * 0.01,\n",
    "            \"b\": np.zeros((1, output_dim))\n",
    "        }\n",
    "\n",
    "    # ReLU activation and derivative\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_deriv(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = [{'a': X}]\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.relu(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer update\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers backward\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            z_curr = self.cache[i+1]['z']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.relu_deriv(z_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8819b501-5aa9-4161-8c22-0a4356992541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(np.argmax(Y_test, axis=1), preds)\n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | Loss={loss:.4f} | Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "392415e5-262c-4f20-8aeb-b357ebf5883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_r(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN_r(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(np.argmax(Y_test, axis=1), preds)\n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | Loss={loss:.4f} | Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a64d325-0db6-4c04-8fbc-6f5c94dc4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_cifar10(\"cifar\")\n",
    "X_test, y_test = load_cifar10_test(\"cifar\")\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "Y_train = enc.fit_transform(y_train)\n",
    "Y_test = enc.transform(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb6313c4-5df6-4f9d-b846-a383b6928940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.3032, Val loss: 2.3032\n",
      "Train acc: 0.1000, Val acc: 0.1000\n",
      "Diagnosis: Underfitting\n"
     ]
    }
   ],
   "source": [
    "def diagnose_model(model, X_train, Y_train, X_val, Y_val):\n",
    "    # Forward passes\n",
    "    out_train = model.forward(X_train)\n",
    "    out_val = model.forward(X_val)\n",
    "\n",
    "    # Compute losses\n",
    "    train_loss = model.loss(Y_train, out_train)\n",
    "    val_loss = model.loss(Y_val, out_val)\n",
    "\n",
    "    # Compute accuracies\n",
    "    train_pred = np.argmax(out_train, axis=1)\n",
    "    val_pred = np.argmax(out_val, axis=1)\n",
    "    y_train_true = np.argmax(Y_train, axis=1)\n",
    "    y_val_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "    train_acc = np.mean(train_pred == y_train_true)\n",
    "    val_acc = np.mean(val_pred == y_val_true)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "    print(f\"Train acc: {train_acc:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Diagnose\n",
    "    if train_acc > 0.9 and val_acc < 0.7:\n",
    "        print(\"Diagnosis: Overfitting\")\n",
    "    elif train_acc < 0.7 and val_acc < 0.7:\n",
    "        print(\"Diagnosis: Underfitting\")\n",
    "    else:\n",
    "        print(\"Diagnosis: Good fit\")\n",
    "\n",
    "# Example usage after training\n",
    "diagnose_model(model, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1cbf0a5-51b0-45a9-8d13-ce33bc9a5b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1layer | Epoch 1/1 | Loss=2.3038 | Acc=0.1000\n",
      "Best accuracy for model_1layer: 0.1000\n"
     ]
    }
   ],
   "source": [
    "model=train_model(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=1, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97e79b5d-3c57-4212-8ee5-ddffb691a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1layer | Epoch 1/1 | Loss=2.3024 | Acc=0.1081\n",
      "Best accuracy for model_1layer: 0.1081\n"
     ]
    }
   ],
   "source": [
    "model_r=train_model_r(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=1, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff2190-88d7-42d2-ab56-ce9397b0379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(X_train, Y_train, X_test, Y_test, [100, 50, 50], \"model_3layer\", epochs=80, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6d824b-f674-4599-8ef2-702863810e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mimo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn 1-layer MLP accuracy: 0.4147 | Training time: 60.0s\n",
      "Sklearn 3-layer MLP accuracy: 0.4409 | Training time: 81.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mimo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model 1: Single hidden layer (100 units)\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', \n",
    "                     solver='sgd', learning_rate_init=0.1, max_iter=50, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "mlp1.fit(X_train_scaled, y_train.ravel())  # scikit-learn expects labels as 1D array\n",
    "end = time.time()\n",
    "preds1 = mlp1.predict(X_test_scaled)\n",
    "acc1 = accuracy_score(y_test, preds1)\n",
    "print(f\"Sklearn 1-layer MLP accuracy: {acc1:.4f} | Training time: {end-start:.1f}s\")\n",
    "\n",
    "# Model 2: Three hidden layers (100, 50, 50)\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(100,50,50), activation='logistic', \n",
    "                     solver='sgd', learning_rate_init=0.1, max_iter=50, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "mlp3.fit(X_train_scaled, y_train.ravel())\n",
    "end = time.time()\n",
    "preds3 = mlp3.predict(X_test_scaled)\n",
    "acc3 = accuracy_score(y_test, preds3)\n",
    "print(f\"Sklearn 3-layer MLP accuracy: {acc3:.4f} | Training time: {end-start:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd9e45-8df7-4e37-b385-32d05cfb4934",
   "metadata": {},
   "source": [
    "## Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
