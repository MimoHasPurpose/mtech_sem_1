{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20f928e-b728-46e3-af52-f92bac1e0be8",
   "metadata": {},
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d235050-b813-4bf0-9d84-22eded7fe747",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4233861-1d43-4c7e-a802-f507fa3334e9",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaea07b7-1dbc-4957-8bee-970f787b95a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.3027\n",
      "Epoch 11: loss=2.3024\n",
      "Epoch 21: loss=2.3021\n",
      "Epoch 31: loss=2.3018\n",
      "Epoch 41: loss=2.3015\n",
      "Epoch 51: loss=2.3011\n",
      "Epoch 61: loss=2.3006\n",
      "Epoch 71: loss=2.2999\n",
      "Epoch 81: loss=2.2989\n",
      "Epoch 91: loss=2.2973\n",
      "Accuracy: 0.1245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# --- Load CIFAR10 ---\n",
    "def load_cifar_batch(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "        X = batch[b'data'].astype(np.float32) / 255.0\n",
    "        y = np.array(batch[b'labels']).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_cifar_batch('cifar/data_batch_1')\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "Y_train = enc.fit_transform(y_train)\n",
    "\n",
    "# --- Network ---\n",
    "class NeuralNet:\n",
    "    def __init__(self, input_size, hidden_layers, hidden_nodes, output_size, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_size\n",
    "        for _ in range(hidden_layers):\n",
    "            self.layers.append({\n",
    "                'W': np.random.randn(prev, hidden_nodes) * 0.01,\n",
    "                'b': np.zeros((1, hidden_nodes))\n",
    "            })\n",
    "            prev = hidden_nodes\n",
    "        self.out = {\n",
    "            'W': np.random.randn(prev, output_size) * 0.01,\n",
    "            'b': np.zeros((1, output_size))\n",
    "        }\n",
    "\n",
    "    def relu(self, z): return np.maximum(0, z)\n",
    "    def relu_deriv(self, z): return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        a = X\n",
    "        self.cache = [{'a': a, 'z': None}]  # store input as layer 0\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.relu(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer gradients\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            z_curr = self.cache[i+1]['z']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.relu_deriv(z_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "# --- Train ---\n",
    "def train_network(num_layers, num_nodes):\n",
    "    model = NeuralNet(3072, num_layers, num_nodes, 10, lr=0.1)\n",
    "    for epoch in range(100):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        if epoch%10==0:\n",
    "            print(f\"Epoch {epoch+1}: loss={loss:.4f}\")\n",
    "    preds = np.argmax(model.forward(X_train), axis=1)\n",
    "    truth = np.argmax(Y_train, axis=1)\n",
    "    acc = np.mean(preds == truth)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "# example\n",
    "train_network(num_layers=2, num_nodes=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7d54c1-7c51-4930-a391-d8cdfbfc47c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1layer | Epoch 1/10 | Loss=2.3033 | Acc=0.0999\n",
      "model_1layer | Epoch 2/10 | Loss=2.3031 | Acc=0.0993\n",
      "model_1layer | Epoch 3/10 | Loss=2.3029 | Acc=0.0983\n",
      "model_1layer | Epoch 4/10 | Loss=2.3027 | Acc=0.1104\n",
      "model_1layer | Epoch 5/10 | Loss=2.3026 | Acc=0.1209\n",
      "model_1layer | Epoch 6/10 | Loss=2.3024 | Acc=0.1249\n",
      "model_1layer | Epoch 7/10 | Loss=2.3023 | Acc=0.1232\n",
      "model_1layer | Epoch 8/10 | Loss=2.3022 | Acc=0.1215\n",
      "model_1layer | Epoch 9/10 | Loss=2.3020 | Acc=0.1209\n",
      "model_1layer | Epoch 10/10 | Loss=2.3019 | Acc=0.1198\n",
      "Best accuracy for model_1layer: 0.1249\n",
      "model_3layer | Epoch 1/10 | Loss=2.3030 | Acc=0.1000\n",
      "model_3layer | Epoch 2/10 | Loss=2.3029 | Acc=0.1000\n",
      "model_3layer | Epoch 3/10 | Loss=2.3028 | Acc=0.1000\n",
      "model_3layer | Epoch 4/10 | Loss=2.3027 | Acc=0.1000\n",
      "model_3layer | Epoch 5/10 | Loss=2.3027 | Acc=0.1000\n",
      "model_3layer | Epoch 6/10 | Loss=2.3027 | Acc=0.1000\n",
      "model_3layer | Epoch 7/10 | Loss=2.3027 | Acc=0.1000\n",
      "model_3layer | Epoch 8/10 | Loss=2.3026 | Acc=0.1000\n",
      "model_3layer | Epoch 9/10 | Loss=2.3026 | Acc=0.1000\n",
      "model_3layer | Epoch 10/10 | Loss=2.3026 | Acc=0.1000\n",
      "Best accuracy for model_3layer: 0.1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ---------- Load CIFAR10 ----------\n",
    "def load_cifar10(path):\n",
    "    data, labels = [], []\n",
    "    for i in range(1, 6):\n",
    "        with open(os.path.join(path, f\"data_batch_{i}\"), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='bytes')\n",
    "            data.append(batch[b'data'])\n",
    "            labels += batch[b'labels']\n",
    "    X = np.concatenate(data).astype(np.float32) / 255.0\n",
    "    y = np.array(labels).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def load_cifar10_test(path):\n",
    "    with open(os.path.join(path, \"test_batch\"), 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "        X = batch[b'data'].astype(np.float32) / 255.0\n",
    "        y = np.array(batch[b'labels']).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# ---------- Network ----------\n",
    "class ANN:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append({\n",
    "                \"W\": np.random.randn(prev, h) * 0.01,\n",
    "                \"b\": np.zeros((1, h))\n",
    "            })\n",
    "            prev = h\n",
    "        self.out = {\n",
    "            \"W\": np.random.randn(prev, output_dim) * 0.01,\n",
    "            \"b\": np.zeros((1, output_dim))\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, z): \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_deriv(self, a): \n",
    "        return a * (1 - a)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = [{'a': X}]\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.sigmoid(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer update\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            a_curr = self.cache[i+1]['a']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.sigmoid_deriv(a_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# ---------- Training ----------\n",
    "def train_model(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(np.argmax(Y_test, axis=1), preds)\n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | Loss={loss:.4f} | Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "X_train, y_train = load_cifar10(\"cifar\")\n",
    "X_test, y_test = load_cifar10_test(\"cifar\")\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "Y_train = enc.fit_transform(y_train)\n",
    "Y_test = enc.transform(y_test)\n",
    "\n",
    "# Train both architectures\n",
    "train_model(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=10, lr=0.1)\n",
    "train_model(X_train, Y_train, X_test, Y_test, [100, 50, 50], \"model_3layer\", epochs=10, lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01a810ef-50d2-4bce-b5da-a74811445f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    train_accs, test_accs = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward and backward on training set\n",
    "        out_train = model.forward(X_train)\n",
    "        loss_train = model.loss(Y_train, out_train)\n",
    "        model.backward(X_train, Y_train, out_train)\n",
    "        \n",
    "        # Training accuracy\n",
    "        preds_train = model.predict(X_train)\n",
    "        acc_train = accuracy_score(np.argmax(Y_train, axis=1), preds_train)\n",
    "        \n",
    "        # Test accuracy\n",
    "        out_test = model.forward(X_test)\n",
    "        loss_test = model.loss(Y_test, out_test)\n",
    "        preds_test = np.argmax(out_test, axis=1)\n",
    "        acc_test = accuracy_score(np.argmax(Y_test, axis=1), preds_test)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(loss_train)\n",
    "        test_losses.append(loss_test)\n",
    "        train_accs.append(acc_train)\n",
    "        test_accs.append(acc_test)\n",
    "        \n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss={loss_train:.4f}, Train Acc={acc_train:.4f} | \"\n",
    "              f\"Test Loss={loss_test:.4f}, Test Acc={acc_test:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if acc_test > best_acc:\n",
    "            best_acc = acc_test\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "    \n",
    "    # Diagnose overfitting/underfitting\n",
    "    if train_accs[-1] < 0.6 and test_accs[-1] < 0.6:\n",
    "        print(\"Model is underfitting\")\n",
    "    elif train_accs[-1] > 0.9 and train_accs[-1] - test_accs[-1] > 0.2:\n",
    "        print(\"Model is overfitting\")\n",
    "    else:\n",
    "        print(\"Model fit seems good\")\n",
    "    \n",
    "    # Optionally, return metrics for plotting\n",
    "    return train_losses, test_losses, train_accs, test_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a106a4d-7d7f-41c3-a24e-a4c0cdaf9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both architectures\n",
    "train_model(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=10, lr=0.1)\n",
    "train_model(X_train, Y_train, X_test, Y_test, [100, 50, 50], \"model_3layer\", epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6d824b-f674-4599-8ef2-702863810e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 8\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m      9\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Model 1: Single hidden layer (100 units)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model 1: Single hidden layer (100 units)\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(100,), activation='logistic', \n",
    "                     solver='sgd', learning_rate_init=0.1, max_iter=50, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "mlp1.fit(X_train_scaled, y_train.ravel())  # scikit-learn expects labels as 1D array\n",
    "end = time.time()\n",
    "preds1 = mlp1.predict(X_test_scaled)\n",
    "acc1 = accuracy_score(y_test, preds1)\n",
    "print(f\"Sklearn 1-layer MLP accuracy: {acc1:.4f} | Training time: {end-start:.1f}s\")\n",
    "\n",
    "# Model 2: Three hidden layers (100, 50, 50)\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(100,50,50), activation='logistic', \n",
    "                     solver='sgd', learning_rate_init=0.1, max_iter=50, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "mlp3.fit(X_train_scaled, y_train.ravel())\n",
    "end = time.time()\n",
    "preds3 = mlp3.predict(X_test_scaled)\n",
    "acc3 = accuracy_score(y_test, preds3)\n",
    "print(f\"Sklearn 3-layer MLP accuracy: {acc3:.4f} | Training time: {end-start:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1137a652-6490-4d6c-9b0b-90a05e99a6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, 4, 1, 1, 2, 7, 0, 3, 4, 7, 7, 2, 9, 9, 9, 3, 3, 6, 4, 3,\n",
       "       6, 6, 2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 2, 0, 3, 7, 3, 0, 5, 2, 2, 7,\n",
       "       1, 1, 1, 2, 2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1,\n",
       "       4, 9, 7, 8, 5, 9, 6, 7, 4, 3, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7,\n",
       "       9, 4, 2, 3, 8, 0, 1, 2, 1, 1, 4, 1, 2, 3, 9, 6, 6, 1, 9, 5, 2, 9,\n",
       "       1, 2, 1, 7, 7, 0, 0, 6, 9, 1, 2, 2, 9, 5, 6, 6, 1, 9, 5, 0, 4, 7,\n",
       "       6, 7, 1, 8, 1, 1, 2, 8, 1, 6, 3, 6, 2, 4, 9, 9, 5, 4, 3, 6, 7, 2,\n",
       "       3, 8, 5, 5, 4, 3, 1, 0, 2, 7, 6, 0, 9, 5, 1, 3, 8, 2, 7, 5, 3, 4,\n",
       "       1, 5, 7, 0, 4, 7, 2, 5, 1, 0, 9, 6, 9, 2, 8, 7, 8, 8, 2, 5, 2, 3,\n",
       "       5, 0, 6, 1, 9, 3, 6, 9, 1, 3, 9, 6, 6, 7, 1, 0, 9, 5, 8, 5, 2, 9,\n",
       "       0, 8, 8, 0, 6, 9, 1, 1, 6, 3, 0, 6, 6, 0, 6, 6, 1, 7, 1, 5, 8, 3,\n",
       "       2, 6, 0, 6, 2, 4, 3, 6, 1, 3, 8, 3, 4, 1, 7, 1, 3, 8, 2, 1, 1, 4,\n",
       "       0, 9, 3, 7, 4, 9, 9, 4, 0, 9, 9, 1, 0, 5, 9, 0, 8, 2, 1, 2, 2, 3,\n",
       "       2, 6, 2, 7, 8, 8, 6, 0, 7, 9, 4, 5, 3, 4, 2, 1, 1, 0, 1, 5, 9, 9,\n",
       "       0, 4, 4, 1, 1, 6, 3, 3, 9, 0, 7, 9, 7, 7, 9, 1, 5, 1, 6, 6, 8, 7,\n",
       "       1, 3, 0, 3, 3, 2, 4, 5, 7, 5, 9, 0, 3, 4, 0, 4, 4, 6, 0, 0, 6, 6,\n",
       "       0, 8, 1, 6, 1, 9, 2, 5, 9, 6, 7, 4, 1, 0, 3, 3, 6, 9, 3, 0, 4, 0,\n",
       "       5, 1, 0, 3, 4, 8, 5, 4, 2, 2, 3, 9, 7, 6, 7, 1, 4, 7, 0, 1, 7, 3,\n",
       "       1, 8, 2, 4, 2, 0, 2, 2, 0, 0, 9, 0, 9, 6, 0, 6, 7, 7, 2, 0, 3, 0,\n",
       "       8, 9, 4, 2, 7, 2, 5, 4, 5, 1, 9, 4, 8, 5, 1, 7, 4, 4, 0, 6, 9, 0,\n",
       "       7, 8, 8, 9, 9, 3, 3, 4, 0, 4, 5, 6, 2, 0, 1, 0, 0, 0, 4, 8, 8, 1,\n",
       "       5, 2, 6, 8, 1, 0, 0, 7, 7, 5, 9, 6, 2, 8, 3, 4, 2, 3, 9, 0, 1, 2,\n",
       "       4, 0, 1, 8, 6, 4, 4, 5, 7, 1, 3, 9, 3, 2, 1, 2, 5, 8, 2, 8, 0, 4,\n",
       "       1, 8, 9, 8, 2, 9, 9, 2, 7, 5, 7, 3, 8, 8, 4, 4, 2, 9, 1, 6, 4, 0,\n",
       "       4, 6, 9, 7, 6, 2, 5, 5, 1, 7, 2, 2, 3, 9, 5, 4, 2, 7, 8, 1, 3, 0,\n",
       "       3, 7, 6, 9, 8, 0, 6, 2, 2, 2, 2, 1, 8, 4, 0, 1, 8, 8, 1, 5, 2, 6,\n",
       "       4, 5, 8, 6, 1, 9, 1, 9, 8, 4, 7, 3, 8, 8, 2, 6, 6, 7, 1, 3, 0, 1,\n",
       "       9, 7, 8, 3, 0, 1, 0, 8, 8, 3, 8, 0, 1, 5, 0, 0, 8, 7, 9, 9, 0, 9,\n",
       "       4, 1, 3, 6, 6, 4, 2, 7, 2, 6, 2, 8, 0, 3, 2, 8, 4, 6, 9, 9, 7, 0,\n",
       "       3, 3, 4, 7, 3, 9, 1, 6, 2, 7, 2, 2, 0, 6, 7, 5, 7, 0, 0, 9, 0, 9,\n",
       "       7, 4, 7, 0, 9, 4, 9, 6, 9, 4, 2, 7, 9, 4, 2, 5, 1, 4, 2, 9, 6, 5,\n",
       "       6, 9, 3, 3, 5, 0, 7, 2, 1, 3, 6, 4, 0, 0, 2, 3, 0, 1, 0, 2, 3, 9,\n",
       "       8, 4, 9, 8, 0, 0, 2, 4, 4, 0, 1, 8, 0, 3, 3, 9, 2, 3, 7, 8, 2, 2,\n",
       "       5, 7, 6, 5, 3, 0, 3, 0, 5, 0, 8, 2, 2, 3, 3, 8, 2, 1, 7, 6, 7, 1,\n",
       "       0, 9, 5, 5, 0, 1, 7, 6, 9, 0, 2, 7, 3, 1, 2, 0, 4, 0, 0, 5, 9, 9,\n",
       "       6, 7, 1, 8, 3, 2, 3, 8, 2, 2, 4, 6, 0, 0, 5, 5, 0, 2, 9, 7, 2, 9,\n",
       "       3, 8, 7, 8, 2, 0, 3, 0, 2, 3, 2, 2, 2, 3, 3, 6, 2, 3, 0, 8, 0, 5,\n",
       "       5, 1, 4, 5, 6, 6, 3, 7, 0, 1, 7, 7, 8, 2, 9, 2, 2, 4, 2, 1, 1, 1,\n",
       "       6, 6, 6, 5, 1, 1, 7, 0, 4, 3, 3, 4, 1, 2, 6, 5, 6, 5, 6, 1, 4, 0,\n",
       "       7, 8, 8, 3, 6, 3, 2, 6, 0, 9, 4, 3, 0, 0, 2, 1, 1, 5, 4, 9, 3, 1,\n",
       "       8, 9, 3, 9, 9, 0, 9, 4, 8, 2, 9, 8, 8, 1, 5, 3, 6, 8, 0, 6, 9, 1,\n",
       "       0, 6, 4, 0, 0, 2, 5, 8, 4, 0, 2, 7, 6, 9, 7, 1, 5, 5, 6, 6, 3, 6,\n",
       "       2, 4, 7, 0, 2, 6, 4, 6, 5, 2, 4, 6, 1, 6, 0, 4, 0, 3, 1, 8, 5, 4,\n",
       "       4, 1, 7, 3, 9, 2, 0, 9, 7, 3, 7, 2, 8, 4, 2, 6, 1, 2, 9, 0, 4, 8,\n",
       "       7, 3, 9, 8, 7, 7, 0, 2, 4, 1, 1, 4, 1, 5, 4, 0, 5, 6, 2, 8, 5, 0,\n",
       "       2, 1, 3, 5, 7, 3, 5, 1, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49bd9e45-8df7-4e37-b385-32d05cfb4934",
   "metadata": {},
   "source": [
    "## Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
