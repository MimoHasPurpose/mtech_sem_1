{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa924e8-2fe3-419c-b5e3-48b54de49169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadf620-9011-41be-b23f-81ccdb1feb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset\n",
    "def load_cifar10(path):\n",
    "    data, labels = [], []\n",
    "    for i in range(1, 6):\n",
    "        with open(os.path.join(path, f\"data_batch_{i}\"), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='bytes')\n",
    "            data.append(batch[b'data'])\n",
    "            labels += batch[b'labels']\n",
    "    X = np.concatenate(data).astype(np.float32) / 255.0\n",
    "    y = np.array(labels).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def load_cifar10_test(path):\n",
    "    with open(os.path.join(path, \"test_batch\"), 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "        X = batch[b'data'].astype(np.float32) / 255.0\n",
    "        y = np.array(batch[b'labels']).reshape(-1, 1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0099cb3-7657-4e42-bb8b-91b406c9ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN \n",
    "class ANN:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append({\n",
    "                \"W\": np.random.randn(prev, h) * 0.01,\n",
    "                \"b\": np.zeros((1, h))\n",
    "            })\n",
    "            prev = h\n",
    "        self.out = {\n",
    "            \"W\": np.random.randn(prev, output_dim) * 0.01,\n",
    "            \"b\": np.zeros((1, output_dim))\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, z): \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_deriv(self, a): \n",
    "        return a * (1 - a)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = [{'a': X}]\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.sigmoid(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer update\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            a_curr = self.cache[i+1]['a']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.sigmoid_deriv(a_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fafc3-a852-40af-b8f3-2f613deaad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_r:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append({\n",
    "                \"W\": np.random.randn(prev, h) * np.sqrt(2.0 / prev),  # He init for ReLU\n",
    "                \"b\": np.zeros((1, h))\n",
    "            })\n",
    "            prev = h\n",
    "        self.out = {\n",
    "            \"W\": np.random.randn(prev, output_dim) * 0.01,\n",
    "            \"b\": np.zeros((1, output_dim))\n",
    "        }\n",
    "\n",
    "    # ReLU activation and derivative\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_deriv(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = [{'a': X}]\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.relu(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer update\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers backward\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            z_curr = self.cache[i+1]['z']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.relu_deriv(z_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48a4f8-17b7-41d5-a4b5-f2aeb7342f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(np.argmax(Y_test, axis=1), preds)\n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | Loss={loss:.4f} | Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409db1fe-9817-4903-a0d6-6d5d07ae5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_r(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN_r(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(np.argmax(Y_test, axis=1), preds)\n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | Loss={loss:.4f} | Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576960-c37b-468a-9ec0-041eb7b4acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_cifar10(\"cifar\")\n",
    "X_test, y_test = load_cifar10_test(\"cifar\")\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "Y_train = enc.fit_transform(y_train)\n",
    "Y_test = enc.transform(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda70765-81e2-400a-8554-e045f01334b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=train_model(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=1, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d1c4e-5553-43aa-a1c4-1e035e38786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r=train_model_r(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=1, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84171f3d-8e5c-4317-a0d8-e787674ba298",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(X_train, Y_train, X_test, Y_test, [100, 50, 50], \"model_3layer\", epochs=80, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a6bd0-4fba-4b77-91a0-bc31e0cc57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_model(model, X_train, Y_train, X_val, Y_val):\n",
    "    # Forward passes\n",
    "    out_train = model.forward(X_train)\n",
    "    out_val = model.forward(X_val)\n",
    "\n",
    "    # Compute losses\n",
    "    train_loss = model.loss(Y_train, out_train)\n",
    "    val_loss = model.loss(Y_val, out_val)\n",
    "\n",
    "    # Compute accuracies\n",
    "    train_pred = np.argmax(out_train, axis=1)\n",
    "    val_pred = np.argmax(out_val, axis=1)\n",
    "    y_train_true = np.argmax(Y_train, axis=1)\n",
    "    y_val_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "    train_acc = np.mean(train_pred == y_train_true)\n",
    "    val_acc = np.mean(val_pred == y_val_true)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "    print(f\"Train acc: {train_acc:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Diagnose\n",
    "    if train_acc > 0.9 and val_acc < 0.7:\n",
    "        print(\"Diagnosis: Overfitting\")\n",
    "    elif train_acc < 0.7 and val_acc < 0.7:\n",
    "        print(\"Diagnosis: Underfitting\")\n",
    "    else:\n",
    "        print(\"Diagnosis: Good fit\")\n",
    "\n",
    "# Example usage after training\n",
    "diagnose_model(model, X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
