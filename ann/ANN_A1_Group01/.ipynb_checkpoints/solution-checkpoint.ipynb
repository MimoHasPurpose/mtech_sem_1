{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f842167-4a06-485f-95e2-17316ed5b2d3",
   "metadata": {},
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8543614-e682-4a09-b854-eed24f00f116",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13005c8b-dd5d-40ca-8b42-54141594eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e633ef8-4e81-48cc-9bb6-f4e517e05675",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0600046-b61d-453d-99a9-de041cb268f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 2.1114, Train Accuracy: 0.2405, Test Accuracy: 0.2315\n",
      "Epoch 2/50, Train Loss: 2.0272, Train Accuracy: 0.2833, Test Accuracy: 0.2786\n",
      "Epoch 3/50, Train Loss: 1.9767, Train Accuracy: 0.3045, Test Accuracy: 0.2984\n",
      "Epoch 4/50, Train Loss: 1.9436, Train Accuracy: 0.3202, Test Accuracy: 0.3161\n",
      "Epoch 5/50, Train Loss: 1.9190, Train Accuracy: 0.3333, Test Accuracy: 0.3280\n",
      "Epoch 6/50, Train Loss: 1.8995, Train Accuracy: 0.3426, Test Accuracy: 0.3371\n",
      "Epoch 7/50, Train Loss: 1.8837, Train Accuracy: 0.3447, Test Accuracy: 0.3401\n",
      "Epoch 8/50, Train Loss: 1.8704, Train Accuracy: 0.3506, Test Accuracy: 0.3464\n",
      "Epoch 9/50, Train Loss: 1.8599, Train Accuracy: 0.3544, Test Accuracy: 0.3514\n",
      "Epoch 10/50, Train Loss: 1.8473, Train Accuracy: 0.3603, Test Accuracy: 0.3520\n",
      "Epoch 11/50, Train Loss: 1.8391, Train Accuracy: 0.3574, Test Accuracy: 0.3538\n",
      "Epoch 12/50, Train Loss: 1.8310, Train Accuracy: 0.3686, Test Accuracy: 0.3643\n",
      "Epoch 13/50, Train Loss: 1.8212, Train Accuracy: 0.3701, Test Accuracy: 0.3668\n",
      "Epoch 14/50, Train Loss: 1.8141, Train Accuracy: 0.3731, Test Accuracy: 0.3674\n",
      "Epoch 15/50, Train Loss: 1.8074, Train Accuracy: 0.3743, Test Accuracy: 0.3680\n",
      "Epoch 16/50, Train Loss: 1.7997, Train Accuracy: 0.3786, Test Accuracy: 0.3738\n",
      "Epoch 17/50, Train Loss: 1.7946, Train Accuracy: 0.3780, Test Accuracy: 0.3719\n",
      "Epoch 18/50, Train Loss: 1.7874, Train Accuracy: 0.3821, Test Accuracy: 0.3757\n",
      "Epoch 19/50, Train Loss: 1.7823, Train Accuracy: 0.3843, Test Accuracy: 0.3780\n",
      "Epoch 20/50, Train Loss: 1.7761, Train Accuracy: 0.3853, Test Accuracy: 0.3821\n",
      "Epoch 21/50, Train Loss: 1.7726, Train Accuracy: 0.3853, Test Accuracy: 0.3791\n",
      "Epoch 22/50, Train Loss: 1.7662, Train Accuracy: 0.3895, Test Accuracy: 0.3837\n",
      "Epoch 23/50, Train Loss: 1.7613, Train Accuracy: 0.3913, Test Accuracy: 0.3844\n",
      "Epoch 24/50, Train Loss: 1.7564, Train Accuracy: 0.3923, Test Accuracy: 0.3877\n",
      "Epoch 25/50, Train Loss: 1.7527, Train Accuracy: 0.3928, Test Accuracy: 0.3875\n",
      "Epoch 26/50, Train Loss: 1.7479, Train Accuracy: 0.3965, Test Accuracy: 0.3915\n",
      "Epoch 27/50, Train Loss: 1.7431, Train Accuracy: 0.3989, Test Accuracy: 0.3938\n",
      "Epoch 28/50, Train Loss: 1.7389, Train Accuracy: 0.4004, Test Accuracy: 0.3937\n",
      "Epoch 29/50, Train Loss: 1.7353, Train Accuracy: 0.3997, Test Accuracy: 0.3958\n",
      "Epoch 30/50, Train Loss: 1.7309, Train Accuracy: 0.4008, Test Accuracy: 0.3972\n",
      "Epoch 31/50, Train Loss: 1.7268, Train Accuracy: 0.4030, Test Accuracy: 0.3987\n",
      "Epoch 32/50, Train Loss: 1.7231, Train Accuracy: 0.4065, Test Accuracy: 0.3992\n",
      "Epoch 33/50, Train Loss: 1.7212, Train Accuracy: 0.4056, Test Accuracy: 0.4003\n",
      "Epoch 34/50, Train Loss: 1.7152, Train Accuracy: 0.4083, Test Accuracy: 0.4047\n",
      "Epoch 35/50, Train Loss: 1.7126, Train Accuracy: 0.4095, Test Accuracy: 0.4042\n",
      "Epoch 36/50, Train Loss: 1.7084, Train Accuracy: 0.4091, Test Accuracy: 0.4067\n",
      "Epoch 37/50, Train Loss: 1.7067, Train Accuracy: 0.4100, Test Accuracy: 0.4031\n",
      "Epoch 38/50, Train Loss: 1.7016, Train Accuracy: 0.4136, Test Accuracy: 0.4088\n",
      "Epoch 39/50, Train Loss: 1.6987, Train Accuracy: 0.4151, Test Accuracy: 0.4102\n",
      "Epoch 40/50, Train Loss: 1.6952, Train Accuracy: 0.4149, Test Accuracy: 0.4101\n",
      "Epoch 41/50, Train Loss: 1.6919, Train Accuracy: 0.4171, Test Accuracy: 0.4121\n",
      "Epoch 42/50, Train Loss: 1.6888, Train Accuracy: 0.4175, Test Accuracy: 0.4108\n",
      "Epoch 43/50, Train Loss: 1.6876, Train Accuracy: 0.4173, Test Accuracy: 0.4092\n",
      "Epoch 44/50, Train Loss: 1.6839, Train Accuracy: 0.4207, Test Accuracy: 0.4149\n",
      "Epoch 45/50, Train Loss: 1.6800, Train Accuracy: 0.4208, Test Accuracy: 0.4154\n",
      "Epoch 46/50, Train Loss: 1.6765, Train Accuracy: 0.4219, Test Accuracy: 0.4173\n",
      "Epoch 47/50, Train Loss: 1.6745, Train Accuracy: 0.4228, Test Accuracy: 0.4147\n",
      "Epoch 48/50, Train Loss: 1.6716, Train Accuracy: 0.4235, Test Accuracy: 0.4165\n",
      "Epoch 49/50, Train Loss: 1.6698, Train Accuracy: 0.4255, Test Accuracy: 0.4176\n",
      "Epoch 50/50, Train Loss: 1.6662, Train Accuracy: 0.4256, Test Accuracy: 0.4197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "def load_cifar10():\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    # Flatten images (32x32x3 = 3072)\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    # One-hot encode labels\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)  # He initialization\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y, output):\n",
    "        # Cross-entropy loss\n",
    "        m = y.shape[0]\n",
    "        log_likelihood = -np.log(output[range(m), np.argmax(y, axis=1)] + 1e-10)\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        # Backward propagation\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = output - y  # Gradient of loss w.r.t. z2\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def train(self, X, y, X_test, y_test, epochs, batch_size, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            # Mini-batch gradient descent\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.forward(X_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(X_batch, y_batch, output, learning_rate)\n",
    "            \n",
    "            # Compute training loss and accuracy\n",
    "            train_output = self.forward(X)\n",
    "            train_loss = self.compute_loss(y, train_output)\n",
    "            train_predictions = np.argmax(train_output, axis=1)\n",
    "            train_labels = np.argmax(y, axis=1)\n",
    "            train_accuracy = np.mean(train_predictions == train_labels)\n",
    "            \n",
    "            # Compute test accuracy\n",
    "            test_output = self.forward(X_test)\n",
    "            test_predictions = np.argmax(test_output, axis=1)\n",
    "            test_labels = np.argmax(y_test, axis=1)\n",
    "            test_accuracy = np.mean(test_predictions == test_labels)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X_train, y_train, X_test, y_test = load_cifar10()\n",
    "    \n",
    "    # Network parameters\n",
    "    input_size = 3072  # 32x32x3\n",
    "    hidden_size = 128  # Number of hidden units\n",
    "    output_size = 10   # Number of classes\n",
    "    epochs = 50\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Initialize and train network\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    nn.train(X_train, y_train, X_test, y_test, epochs, batch_size, learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456e7cb-0da8-48fe-a8f5-c744a3e9fac7",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Data Preprocessing** (`load_cifar10`):\n",
    "   - **Loading**: Uses Keras to load CIFAR-10 (50,000 training images, 10,000 test images).\n",
    "   - **Normalization**: Scales pixel values from [0, 255] to [0, 1].\n",
    "   - **Flattening**: Reshapes each 32x32x3 image to a 3072-dimensional vector.\n",
    "   - **One-hot Encoding**: Converts labels to one-hot vectors (e.g., class 3 → [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]).\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - **ReLU**: `max(0, x)` for the hidden layer, with derivative 1 for x > 0, else 0.\n",
    "   - **Softmax**: Converts output logits to probabilities, ensuring they sum to 1. Subtracts the maximum value for numerical stability.\n",
    "\n",
    "3. **Neural Network Architecture** (`NeuralNetwork` class):\n",
    "   - **Initialization**:\n",
    "     - Input layer: 3072 units (flattened image).\n",
    "     - Hidden layer: 128 units with ReLU activation.\n",
    "     - Output layer: 10 units with softmax activation.\n",
    "     - Weights are initialized using He initialization (`sqrt(2/input_size)`) for better training stability.\n",
    "     - Biases are initialized to zeros.\n",
    "   - **Forward Propagation**:\n",
    "     - Computes `z1 = X * W1 + b1`, then `a1 = ReLU(z1)`.\n",
    "     - Computes `z2 = a1 * W2 + b2`, then `a2 = softmax(z2)`.\n",
    "   - **Loss Computation**:\n",
    "     - Uses cross-entropy loss: `-sum(y * log(output))`, averaged over the batch.\n",
    "     - Adds a small constant (`1e-10`) to avoid log(0).\n",
    "   - **Backward Propagation**:\n",
    "     - Output layer: Computes gradient `dz2 = output - y` (softmax + cross-entropy derivative).\n",
    "     - Hidden layer: Backpropagates error using `dz1 = (dz2 * W2.T) * ReLU'(z1)`.\n",
    "     - Computes gradients for weights (`dW1`, `dW2`) and biases (`db1`, `db2`).\n",
    "     - Updates parameters using gradient descent: `W = W - learning_rate * dW`.\n",
    "   - **Training**:\n",
    "     - Implements mini-batch gradient descent with batch size 128.\n",
    "     - Shuffles training data each epoch to improve generalization.\n",
    "     - Runs for 20 epochs with a learning rate of 0.001.\n",
    "     - Reports training loss, training accuracy, and test accuracy per epoch.\n",
    "\n",
    "4. **Training and Evaluation**:\n",
    "   - Trains on the full training set, evaluates on both training and test sets.\n",
    "   - Accuracy is computed by comparing predicted class (argmax of output) to true class.\n",
    "\n",
    "### Notes\n",
    "- **Performance**: This simple ANN typically achieves 40-50% test accuracy on CIFAR-10 due to its basic architecture. For better performance, consider:\n",
    "  - Adding more hidden layers or units.\n",
    "  - Using convolutional neural networks (CNNs), which are more suited for image data.\n",
    "  - Implementing regularization (e.g., dropout, L2).\n",
    "  - Using advanced optimizers (e.g., Adam).\n",
    "- **Hyperparameters**:\n",
    "  - Hidden size (128), batch size (128), learning rate (0.001), and epochs (20) are chosen for simplicity. Tune these for better results.\n",
    "- **Dependencies**: Requires NumPy and TensorFlow (for CIFAR-10 loading only).\n",
    "- **Runtime**: Training may take a few minutes on a CPU due to the dataset size and matrix operations.\n",
    "\n",
    "This implementation is a minimal, from-scratch ANN to demonstrate core concepts. For production use, frameworks like TensorFlow or PyTorch are recommended for efficiency and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8f57f-cbb9-4c11-b7a4-443223950841",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d4969-5f08-4cac-b2ef-d0b1fa188f3b",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72842b38-5653-4cea-99e4-4e834466eed9",
   "metadata": {},
   "source": [
    "## Further experiments:\n",
    "- test on different dataset\n",
    "- test with different activation functions\n",
    "- test with different gpus and calculate time\n",
    "- use diffeent regularization\n",
    "- use advanced optimizers\n",
    "- try on different depth of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad7384-666d-4142-9f0b-ddd44248bf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
