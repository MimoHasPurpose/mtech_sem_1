{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa924e8-2fe3-419c-b5e3-48b54de49169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fadf620-9011-41be-b23f-81ccdb1feb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset\n",
    "def load_cifar10(path):\n",
    "    data, labels = [], []\n",
    "    for i in range(1, 6):\n",
    "        with open(os.path.join(path, f\"data_batch_{i}\"), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='bytes')\n",
    "            data.append(batch[b'data'])\n",
    "            labels += batch[b'labels']\n",
    "    X = np.concatenate(data).astype(np.float32) / 255.0\n",
    "    y = np.array(labels).reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "def load_cifar10_test(path):\n",
    "    with open(os.path.join(path, \"test_batch\"), 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "        X = batch[b'data'].astype(np.float32) / 255.0\n",
    "        y = np.array(batch[b'labels']).reshape(-1, 1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0099cb3-7657-4e42-bb8b-91b406c9ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN \n",
    "class ANN:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append({\n",
    "                \"W\": np.random.randn(prev, h) * 0.01,\n",
    "                \"b\": np.zeros((1, h))\n",
    "            })\n",
    "            prev = h\n",
    "        self.out = {\n",
    "            \"W\": np.random.randn(prev, output_dim) * 0.01,\n",
    "            \"b\": np.zeros((1, output_dim))\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, z): \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_deriv(self, a): \n",
    "        return a * (1 - a)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = [{'a': X}]\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.sigmoid(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer update\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            a_curr = self.cache[i+1]['a']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.sigmoid_deriv(a_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1fafc3-a852-40af-b8f3-2f613deaad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_r:\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_layers:\n",
    "            self.layers.append({\n",
    "                \"W\": np.random.randn(prev, h) * np.sqrt(2.0 / prev),  # He init for ReLU\n",
    "                \"b\": np.zeros((1, h))\n",
    "            })\n",
    "            prev = h\n",
    "        self.out = {\n",
    "            \"W\": np.random.randn(prev, output_dim) * 0.01,\n",
    "            \"b\": np.zeros((1, output_dim))\n",
    "        }\n",
    "\n",
    "    # ReLU activation and derivative\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_deriv(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z, axis=1, keepdims=True)\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = [{'a': X}]\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            z = a @ layer['W'] + layer['b']\n",
    "            a = self.relu(z)\n",
    "            self.cache.append({'a': a, 'z': z})\n",
    "        z_out = a @ self.out['W'] + self.out['b']\n",
    "        a_out = self.softmax(z_out)\n",
    "        self.cache.append({'a': a_out, 'z': z_out})\n",
    "        return a_out\n",
    "\n",
    "    def backward(self, X, Y, out):\n",
    "        m = X.shape[0]\n",
    "        dz = (out - Y) / m\n",
    "\n",
    "        # output layer update\n",
    "        a_prev = self.cache[-2]['a']\n",
    "        dW = a_prev.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        self.out['W'] -= self.lr * dW\n",
    "        self.out['b'] -= self.lr * db\n",
    "        da = dz @ self.out['W'].T\n",
    "\n",
    "        # hidden layers backward\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            z_curr = self.cache[i+1]['z']\n",
    "            a_prev = self.cache[i]['a']\n",
    "            dz = da * self.relu_deriv(z_curr)\n",
    "            dW = a_prev.T @ dz\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            self.layers[i]['W'] -= self.lr * dW\n",
    "            self.layers[i]['b'] -= self.lr * db\n",
    "            da = dz @ self.layers[i]['W'].T\n",
    "\n",
    "    def loss(self, Y, out):\n",
    "        return -np.mean(np.sum(Y * np.log(out + 1e-9), axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d48a4f8-17b7-41d5-a4b5-f2aeb7342f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(np.argmax(Y_test, axis=1), preds)\n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | Loss={loss:.4f} | Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409db1fe-9817-4903-a0d6-6d5d07ae5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_r(X_train, Y_train, X_test, Y_test, layers, name, epochs=10, lr=0.1):\n",
    "    model = ANN_r(3072, layers, 10, lr)\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        out = model.forward(X_train)\n",
    "        loss = model.loss(Y_train, out)\n",
    "        model.backward(X_train, Y_train, out)\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(np.argmax(Y_test, axis=1), preds)\n",
    "        print(f\"{name} | Epoch {epoch+1}/{epochs} | Loss={loss:.4f} | Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            np.savez(f\"{name}_best_weights.npz\",\n",
    "                     layers=model.layers, out=model.out)\n",
    "    print(f\"Best accuracy for {name}: {best_acc:.4f}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca576960-c37b-468a-9ec0-041eb7b4acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_cifar10(\"cifar\")\n",
    "X_test, y_test = load_cifar10_test(\"cifar\")\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "Y_train = enc.fit_transform(y_train)\n",
    "Y_test = enc.transform(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda70765-81e2-400a-8554-e045f01334b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1layer | Epoch 1/40 | Loss=2.3042 | Acc=0.1140\n",
      "model_1layer | Epoch 2/40 | Loss=2.3039 | Acc=0.1154\n",
      "model_1layer | Epoch 3/40 | Loss=2.3036 | Acc=0.1165\n",
      "model_1layer | Epoch 4/40 | Loss=2.3034 | Acc=0.1171\n",
      "model_1layer | Epoch 5/40 | Loss=2.3032 | Acc=0.1186\n",
      "model_1layer | Epoch 6/40 | Loss=2.3030 | Acc=0.1189\n",
      "model_1layer | Epoch 7/40 | Loss=2.3029 | Acc=0.1188\n",
      "model_1layer | Epoch 8/40 | Loss=2.3027 | Acc=0.1229\n",
      "model_1layer | Epoch 9/40 | Loss=2.3026 | Acc=0.1263\n",
      "model_1layer | Epoch 10/40 | Loss=2.3025 | Acc=0.1312\n",
      "model_1layer | Epoch 11/40 | Loss=2.3024 | Acc=0.1343\n",
      "model_1layer | Epoch 12/40 | Loss=2.3023 | Acc=0.1345\n",
      "model_1layer | Epoch 13/40 | Loss=2.3022 | Acc=0.1390\n",
      "model_1layer | Epoch 14/40 | Loss=2.3021 | Acc=0.1394\n",
      "model_1layer | Epoch 15/40 | Loss=2.3021 | Acc=0.1427\n",
      "model_1layer | Epoch 16/40 | Loss=2.3020 | Acc=0.1434\n",
      "model_1layer | Epoch 17/40 | Loss=2.3019 | Acc=0.1425\n",
      "model_1layer | Epoch 18/40 | Loss=2.3018 | Acc=0.1440\n",
      "model_1layer | Epoch 19/40 | Loss=2.3017 | Acc=0.1434\n",
      "model_1layer | Epoch 20/40 | Loss=2.3017 | Acc=0.1451\n",
      "model_1layer | Epoch 21/40 | Loss=2.3016 | Acc=0.1455\n",
      "model_1layer | Epoch 22/40 | Loss=2.3015 | Acc=0.1463\n",
      "model_1layer | Epoch 23/40 | Loss=2.3014 | Acc=0.1491\n",
      "model_1layer | Epoch 24/40 | Loss=2.3013 | Acc=0.1490\n",
      "model_1layer | Epoch 25/40 | Loss=2.3013 | Acc=0.1507\n",
      "model_1layer | Epoch 26/40 | Loss=2.3012 | Acc=0.1534\n",
      "model_1layer | Epoch 27/40 | Loss=2.3011 | Acc=0.1551\n",
      "model_1layer | Epoch 28/40 | Loss=2.3010 | Acc=0.1569\n",
      "model_1layer | Epoch 29/40 | Loss=2.3009 | Acc=0.1598\n",
      "model_1layer | Epoch 30/40 | Loss=2.3009 | Acc=0.1625\n",
      "model_1layer | Epoch 31/40 | Loss=2.3008 | Acc=0.1640\n",
      "model_1layer | Epoch 32/40 | Loss=2.3007 | Acc=0.1670\n",
      "model_1layer | Epoch 33/40 | Loss=2.3006 | Acc=0.1703\n",
      "model_1layer | Epoch 34/40 | Loss=2.3005 | Acc=0.1731\n",
      "model_1layer | Epoch 35/40 | Loss=2.3004 | Acc=0.1753\n",
      "model_1layer | Epoch 36/40 | Loss=2.3004 | Acc=0.1778\n",
      "model_1layer | Epoch 37/40 | Loss=2.3003 | Acc=0.1806\n",
      "model_1layer | Epoch 38/40 | Loss=2.3002 | Acc=0.1826\n",
      "model_1layer | Epoch 39/40 | Loss=2.3001 | Acc=0.1858\n",
      "model_1layer | Epoch 40/40 | Loss=2.3000 | Acc=0.1883\n",
      "Best accuracy for model_1layer: 0.1883\n"
     ]
    }
   ],
   "source": [
    "model=train_model(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=0, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "267d1c4e-5553-43aa-a1c4-1e035e38786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1layer | Epoch 1/40 | Loss=2.3081 | Acc=0.1132\n",
      "model_1layer | Epoch 2/40 | Loss=2.3012 | Acc=0.1239\n",
      "model_1layer | Epoch 3/40 | Loss=2.2963 | Acc=0.1182\n",
      "model_1layer | Epoch 4/40 | Loss=2.2922 | Acc=0.1183\n",
      "model_1layer | Epoch 5/40 | Loss=2.2886 | Acc=0.1191\n",
      "model_1layer | Epoch 6/40 | Loss=2.2851 | Acc=0.1234\n",
      "model_1layer | Epoch 7/40 | Loss=2.2815 | Acc=0.1269\n",
      "model_1layer | Epoch 8/40 | Loss=2.2778 | Acc=0.1330\n",
      "model_1layer | Epoch 9/40 | Loss=2.2739 | Acc=0.1386\n",
      "model_1layer | Epoch 10/40 | Loss=2.2697 | Acc=0.1462\n",
      "model_1layer | Epoch 11/40 | Loss=2.2652 | Acc=0.1566\n",
      "model_1layer | Epoch 12/40 | Loss=2.2604 | Acc=0.1667\n",
      "model_1layer | Epoch 13/40 | Loss=2.2552 | Acc=0.1734\n",
      "model_1layer | Epoch 14/40 | Loss=2.2497 | Acc=0.1795\n",
      "model_1layer | Epoch 15/40 | Loss=2.2439 | Acc=0.1867\n",
      "model_1layer | Epoch 16/40 | Loss=2.2378 | Acc=0.1913\n",
      "model_1layer | Epoch 17/40 | Loss=2.2313 | Acc=0.1978\n",
      "model_1layer | Epoch 18/40 | Loss=2.2246 | Acc=0.2022\n",
      "model_1layer | Epoch 19/40 | Loss=2.2175 | Acc=0.2068\n",
      "model_1layer | Epoch 20/40 | Loss=2.2102 | Acc=0.2113\n",
      "model_1layer | Epoch 21/40 | Loss=2.2028 | Acc=0.2135\n",
      "model_1layer | Epoch 22/40 | Loss=2.1951 | Acc=0.2173\n",
      "model_1layer | Epoch 23/40 | Loss=2.1875 | Acc=0.2211\n",
      "model_1layer | Epoch 24/40 | Loss=2.1797 | Acc=0.2244\n",
      "model_1layer | Epoch 25/40 | Loss=2.1721 | Acc=0.2257\n",
      "model_1layer | Epoch 26/40 | Loss=2.1645 | Acc=0.2275\n",
      "model_1layer | Epoch 27/40 | Loss=2.1570 | Acc=0.2300\n",
      "model_1layer | Epoch 28/40 | Loss=2.1497 | Acc=0.2335\n",
      "model_1layer | Epoch 29/40 | Loss=2.1426 | Acc=0.2363\n",
      "model_1layer | Epoch 30/40 | Loss=2.1358 | Acc=0.2379\n",
      "model_1layer | Epoch 31/40 | Loss=2.1291 | Acc=0.2385\n",
      "model_1layer | Epoch 32/40 | Loss=2.1227 | Acc=0.2420\n",
      "model_1layer | Epoch 33/40 | Loss=2.1165 | Acc=0.2452\n",
      "model_1layer | Epoch 34/40 | Loss=2.1105 | Acc=0.2470\n",
      "model_1layer | Epoch 35/40 | Loss=2.1048 | Acc=0.2474\n",
      "model_1layer | Epoch 36/40 | Loss=2.0992 | Acc=0.2485\n",
      "model_1layer | Epoch 37/40 | Loss=2.0938 | Acc=0.2505\n",
      "model_1layer | Epoch 38/40 | Loss=2.0886 | Acc=0.2533\n",
      "model_1layer | Epoch 39/40 | Loss=2.0836 | Acc=0.2540\n",
      "model_1layer | Epoch 40/40 | Loss=2.0787 | Acc=0.2559\n",
      "Best accuracy for model_1layer: 0.2559\n"
     ]
    }
   ],
   "source": [
    "model2=train_model_r(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=40, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ac214e1-1927-49c6-bc83-250d92c58816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1layer | Epoch 1/40 | Loss=2.3042 | Acc=0.1216\n",
      "model_1layer | Epoch 2/40 | Loss=2.2958 | Acc=0.1169\n",
      "model_1layer | Epoch 3/40 | Loss=2.2882 | Acc=0.1265\n",
      "model_1layer | Epoch 4/40 | Loss=2.2797 | Acc=0.1503\n",
      "model_1layer | Epoch 5/40 | Loss=2.2691 | Acc=0.1727\n",
      "model_1layer | Epoch 6/40 | Loss=2.2558 | Acc=0.1905\n",
      "model_1layer | Epoch 7/40 | Loss=2.2389 | Acc=0.2057\n",
      "model_1layer | Epoch 8/40 | Loss=2.2185 | Acc=0.2191\n",
      "model_1layer | Epoch 9/40 | Loss=2.1953 | Acc=0.2268\n",
      "model_1layer | Epoch 10/40 | Loss=2.1709 | Acc=0.2413\n",
      "model_1layer | Epoch 11/40 | Loss=2.1477 | Acc=0.2205\n",
      "model_1layer | Epoch 12/40 | Loss=2.1428 | Acc=0.1115\n",
      "model_1layer | Epoch 13/40 | Loss=2.5801 | Acc=0.1015\n",
      "model_1layer | Epoch 14/40 | Loss=6.1075 | Acc=0.1000\n",
      "model_1layer | Epoch 15/40 | Loss=17.0865 | Acc=0.1000\n",
      "model_1layer | Epoch 16/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 17/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 18/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 19/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 20/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 21/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 22/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 23/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 24/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 25/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 26/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 27/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 28/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 29/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 30/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 31/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 32/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 33/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 34/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 35/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 36/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 37/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 38/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 39/40 | Loss=18.6509 | Acc=0.1000\n",
      "model_1layer | Epoch 40/40 | Loss=18.6509 | Acc=0.1000\n",
      "Best accuracy for model_1layer: 0.2413\n"
     ]
    }
   ],
   "source": [
    "model3=train_model_r(X_train, Y_train, X_test, Y_test, [100], \"model_1layer\", epochs=40, lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b8a6bd0-4fba-4b77-91a0-bc31e0cc57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_model(model, X_train, Y_train, X_val, Y_val):\n",
    "    # Forward passes\n",
    "    out_train = model.forward(X_train)\n",
    "    out_val = model.forward(X_val)\n",
    "\n",
    "    # Compute losses\n",
    "    train_loss = model.loss(Y_train, out_train)\n",
    "    val_loss = model.loss(Y_val, out_val)\n",
    "\n",
    "    # Compute accuracies\n",
    "    train_pred = np.argmax(out_train, axis=1)\n",
    "    val_pred = np.argmax(out_val, axis=1)\n",
    "    y_train_true = np.argmax(Y_train, axis=1)\n",
    "    y_val_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "    train_acc = np.mean(train_pred == y_train_true)\n",
    "    val_acc = np.mean(val_pred == y_val_true)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "    print(f\"Train acc: {train_acc:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Diagnose\n",
    "    if train_acc > 0.9 and val_acc < 0.7:\n",
    "        print(\"Diagnosis: Overfitting\")\n",
    "    elif train_acc < 0.7 and val_acc < 0.7:\n",
    "        print(\"Diagnosis: Underfitting\")\n",
    "    else:\n",
    "        print(\"Diagnosis: Good fit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c7560b-e396-465d-a3e8-49a6306fa2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.0740, Val loss: 2.0745\n",
      "Train acc: 0.2572, Val acc: 0.2559\n",
      "Diagnosis: Underfitting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "diagnose_model(model, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d51b9c82-ba35-48f7-afd8-9a869b9e52c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.0740, Val loss: 2.0745\n",
      "Train acc: 0.2572, Val acc: 0.2559\n",
      "Diagnosis: Underfitting\n"
     ]
    }
   ],
   "source": [
    "diagnose_model(model2, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae292f7c-1444-4172-bcaf-67e648471e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 18.6509, Val loss: 18.6509\n",
      "Train acc: 0.1000, Val acc: 0.1000\n",
      "Diagnosis: Underfitting\n"
     ]
    }
   ],
   "source": [
    "diagnose_model(model3, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac20f9d-67ab-4583-97e9-6907a95667a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
